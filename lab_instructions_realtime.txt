Link to the dataset - https://www.kaggle.com/carrie1/ecommerce-data
lambda - WriteKinesis >> Python 3.8
api gateway - Orders
kinesis - APIData
s3 - raworders-ekas
lambda - Write-Kinesis-To-s3
dynamodb - Customers >> CustomerID
           Invoices >> InvoiceNo
lambda - Write-To-DynamoDB
redshift - orders
s3 - firehouseredshift-eka
firehouse - RedshiftDelivery

1. Create a lambda function to write to kinesis.
2. Create a rest api gateway and point to lambda function above.
--create resources
--create method
3. Once created, configure the connection to the lambda function. While being on the Get method, select Lambda function and point to your function. 
Once configured, in order lambda to receive the values, change the mapping template. Click Integration request and then, Mapping Templates and change the template to 'Method Request passthrough'.
4. Deploy your API to generate the API url.
5. Create Kinesis data stream.
6. Make sure that lambda can write to kinesis and add policies:
--kinesisfullaccess - standard
--mygetdynamodb - attach policy
--mykinesisread - attach policy --put records and put record
--mykinesiswriteapidata - attach policy
--awslambdabasicexecutionrole - attach policy
7. To lambda, add the code from the folder in this Git repository.
8. Initiate the code from the folder in this Git repository to send records to API.
9. Create a S3 bucket to stream data from Kinesis.
10. Create an IAM role for the newly lambda function.
--s3fullaccess
--mykinesisread
--awslambdabasicexecutionrole
11. Create a lambda function >> blueprint - kinesis-process-record-python to write to s3 and add the lambda code from the folder in this Git repository.
12. Create dynamodb tables.
13. Create an IAM role for the lambda function.
--awslambdabasicexecutionrole
--mykinesis2
--mydynamodbwrite
14. Create a lambda function to write to dynamodb.
15. Test with the get API gateway or Postman.
16. Create an elastic IP to allow instances and devices outside of VPC connect to the database.
16. Configure redshift cluster and change the settings to publicy accessible.
17. Setup VPC routing for firehouse i.e. make sure that firehouse is in inbound rules - {FirehouseIP}/27. You can find firehouse IP at https://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-rs-vpc, e.g. 52.70.63.192/27 for N.Virginia.
18. Create tables in redshift in Editor using the code in the folder in this repository.
19. Create a s3 bucket and upload jsonpath file in the folder in this repository.
20. Configure firehouse - add 'json 's3://firehouseredshift-eka/jsonpaths.json';' to COPY options - optional
21. Open port 5432 as an inbound rule in Redshift and setup Power BI to query data.